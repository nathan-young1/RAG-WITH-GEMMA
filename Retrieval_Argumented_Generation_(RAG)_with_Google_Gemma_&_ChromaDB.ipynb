{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathan-young1/RAG-WITH-GEMMA/blob/main/Retrieval_Argumented_Generation_(RAG)_with_Google_Gemma_%26_ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG)\n",
        "\n",
        "Large language models (LLMs) like Gemma are great at understanding language and generating fluent text. However, sometimes they struggle with factual accuracy or keeping information up to date. Retrieval augmented generation (RAG) solves this by adding a \"research assistant\" step:\n",
        "\n",
        "1. **Retrieval**: When you give the LLM a prompt or question, RAG first searches through a database of texts üìú - like having access to a giant virtual library! It retrieves relevant snippets of information that could be useful for composing its response.\n",
        "\n",
        "2. **Augmentation**: Those retrieved context passages are then incorporated into the prompt to the LLM üìù, giving it an information source to base the answer on. Just like reading research notes from a database and integrating them into your understanding before writing on a topic.\n",
        "\n",
        "3. **Generation**: Finally, the LLM leverages the augmented context to expand its knowledge and language capabilities to generate a response. Making the text produced not just fluent, but also accurate and factual, since it's based on relevant reference material.\n",
        "\n",
        "In essence, RAG reduces the LLM's chance of hallucinating because now it gets to consult a knowledge base before responding. This makes responses more reliable and trustworthy, especially for topics requiring specific up-to-date facts."
      ],
      "metadata": {
        "id": "tk4fMl1KiE5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://python.langchain.com/assets/images/vector_stores-125d1675d58cfb46ce9054c9019fea72.jpg\" height=400 width=800/>\n",
        "\n",
        "‚≠ê Photo credits: [Langchain](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ],
      "metadata": {
        "id": "-KsvDD0jiE53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieval**\n",
        "\n",
        "To use RAG, we need to have a database of documents that can provide relevant information for our queries. In this tutorial, we will create a database from google gemini privacy policy & support page as at 18th April 2024 (note i have already converted the webpage to pdf for you). We will use Langchain, ChromaDB, and Hugging Face to perform RAG on this book.\n",
        "\n",
        "The process of creating a database involves the following steps:\n",
        "\n",
        "- **Chunking**: We divide the book (pdf in this case) into smaller pieces, such as paragraphs or sentences, that can be easily indexed and retrieved.\n",
        "\n",
        "- **Embedding**: We use a pre-trained model from Hugging Face to convert each chunk into a vector representation, also known as a sentence embedding. This captures the semantic meaning of the chunk and allows us to compare it with other chunks or queries.\n",
        "\n",
        "üí°: For more information on vector embeddings check out the word embeddings section in my last lesson at [Notebook Link](https://www.kaggle.com/code/nathanyoung1/transformer-based-language-translation-in-pytorch). The word embeddings are combined to form sentence embeddings which we will refer to as vector embeddings throughout this tutorial.\n",
        "\n",
        "- **Indexing**: We store the vector embeddings in a vector database, such as ChromaDB, that can efficiently perform similarity search. This means that given a query vector, we can find the most similar vectors in the database, and retrieve the corresponding chunks.\n",
        "\n",
        "When we want to use RAG to generate a response for a query, we first embed the query using the same model as before. Then, as shown in the image above üëÜ we use the vector database to find the most similar embeddings to the query embedding. These similar embeddings are linked to particular chunks of our document. We then fed this chunks as context to the LLM, enabling it to generate a coherent and informative answer."
      ],
      "metadata": {
        "id": "m_WRX5EaiE54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the vector database, langchain, hugging face sentence_transformers\n",
        "!pip install --quiet chromadb langchain sentence_transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "10wKzYNUiE55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Gemini privacy policy & Support from GDrive Link"
      ],
      "metadata": {
        "id": "6YDm2T_WNOtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown"
      ],
      "metadata": {
        "id": "k88Ku7frLyqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url_tofile = \"https://drive.google.com/uc?export=download&id=1MW2boTI9PEsobl5Kn0yDKbIXBaTCBOTg\"\n",
        "\n",
        "# download\n",
        "gdown.download(url_tofile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "vhJYNUqHNxUQ",
        "outputId": "70c39fb8-cb7c-4de7-843c-016dee068a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1MW2boTI9PEsobl5Kn0yDKbIXBaTCBOTg\n",
            "To: /content/google gemini privacy policy and support @ 18 April 2024.pdf\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.31M/1.31M [00:00<00:00, 46.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'google gemini privacy policy and support @ 18 April 2024.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieval Cont'd"
      ],
      "metadata": {
        "id": "82gcgwSDNqUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pdfminer pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc6JutjAc1Ri",
        "outputId": "7c39141f-9433-4321-853b-6459ae85c7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader\n",
        "\n",
        "# Load the entire pdf file as one document.\n",
        "loader = PDFMinerLoader(\"/content/google gemini privacy policy and support @ 18 April 2024.pdf\")\n",
        "entire_pdf_asdocument = loader.load()[0] # there is only one documents, so am getting it."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:13:58.226771Z",
          "iopub.execute_input": "2024-02-16T01:13:58.227146Z",
          "iopub.status.idle": "2024-02-16T01:14:00.477452Z",
          "shell.execute_reply.started": "2024-02-16T01:13:58.22711Z",
          "shell.execute_reply": "2024-02-16T01:14:00.476578Z"
        },
        "trusted": true,
        "id": "eHJEA7G8iE57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "# Load an embedding model from hugging face.\n",
        "embed_model = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:14:02.502954Z",
          "iopub.execute_input": "2024-02-16T01:14:02.503662Z",
          "iopub.status.idle": "2024-02-16T01:14:20.072514Z",
          "shell.execute_reply.started": "2024-02-16T01:14:02.503627Z",
          "shell.execute_reply": "2024-02-16T01:14:20.071661Z"
        },
        "trusted": true,
        "id": "AAfaxiZyiE57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Explanation: The RecursiveCharacterTextSplitter breaks down a large text into smaller chunks.\n",
        "# It uses a predefined set of characters (like spaces, newlines, etc.) to split the text.\n",
        "# The goal is to find the best split that balances size and meaningful content.\n",
        "\n",
        "# Create an instance of the RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to demonstrate.\n",
        "    chunk_size=512,  # Given that an average word is 5 letters... approximately 102 words\n",
        "    chunk_overlap=102,  # Approximately 20% overlap (around 20 words).\n",
        "    is_separator_regex=False,  # Indicates that separators are not regular expressions.\n",
        ")\n",
        "\n",
        "# use the text splitter to split into chunks\n",
        "chunks = text_splitter.create_documents([entire_pdf_asdocument.page_content])"
      ],
      "metadata": {
        "trusted": true,
        "id": "ewW7Tt98iE58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# embed and insert all chunks of the documents into the vector database\n",
        "vector_db = Chroma.from_documents(\n",
        "    chunks,\n",
        "    embed_model, # model to use for embedding the document chunks before storing.\n",
        "    persist_directory='vector_db', # persist the database in memory.\n",
        "    collection_name='gemini_privacy_book' # name of the collection to store the chunks in.\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:15:38.832354Z",
          "iopub.execute_input": "2024-02-16T01:15:38.832711Z",
          "iopub.status.idle": "2024-02-16T01:15:42.705172Z",
          "shell.execute_reply.started": "2024-02-16T01:15:38.83268Z",
          "shell.execute_reply": "2024-02-16T01:15:42.704368Z"
        },
        "trusted": true,
        "id": "nB0v5zgniE59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a vector similarity search on a query.\n",
        "query = \"what do you do with my images ?\"\n",
        "\n",
        "# return the chunks of the most similar five embeddings in the db\n",
        "chunks_retrieved = vector_db.similarity_search(query, k=1)\n",
        "\n",
        "print(chunks_retrieved[0].page_content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:15:49.246632Z",
          "iopub.execute_input": "2024-02-16T01:15:49.247473Z",
          "iopub.status.idle": "2024-02-16T01:15:49.327884Z",
          "shell.execute_reply.started": "2024-02-16T01:15:49.247432Z",
          "shell.execute_reply": "2024-02-16T01:15:49.326824Z"
        },
        "trusted": true,
        "id": "evGmPItRiE59",
        "outputId": "6737476e-c9cc-4e5d-851c-a7fa79e34ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "How does Google work with my uploaded images?\n",
            "\n",
            "How images in prompts work\n",
            "\n",
            "When you add an image to your prompt, Gemini Apps use Google Lens technology\n",
            "to understand what's in the image. For example, Google Lens might interpret an\n",
            "image's pixels as a cat jumping. Gemini Apps add this information to your prompt\n",
            "to understand your request better. Google uses this information just like any other\n",
            "prompt, as explained in the Gemini Apps Privacy Notice.\n",
            "\n",
            "How we limit use of your actual images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Argumentation** ‚ûï\n",
        "\n",
        "Now that we have setup a vector database and can retrieve similar chunks to our query, we are going to combine this chunks together to form a context. This context is then passed together with our query as the prompt to our LLM."
      ],
      "metadata": {
        "id": "57DBvbE8iE59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# util function to join all retrieved documents chunks together to form a context.\n",
        "def join_chunks(chunks_retrieved):\n",
        "    return \"\\n\\n\".join([chunk.page_content for chunk in chunks_retrieved])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:16:19.258198Z",
          "iopub.execute_input": "2024-02-16T01:16:19.258591Z",
          "iopub.status.idle": "2024-02-16T01:16:19.263506Z",
          "shell.execute_reply.started": "2024-02-16T01:16:19.25856Z",
          "shell.execute_reply": "2024-02-16T01:16:19.262397Z"
        },
        "trusted": true,
        "id": "e1qIOw7SiE5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generation** ‚úçÔ∏è"
      ],
      "metadata": {
        "id": "-gilvu95iE5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt to condition the LLM's behavior.\n",
        "system_prompt = \"\"\"\n",
        "You are an AI assistant specializing in Google Gemini's privacy policy. Your role is to help users understand and navigate the privacy policy by answering their questions based on the provided context.\n",
        "\n",
        "When given a context and a question related to the privacy policy:\n",
        "\n",
        "Carefully read and comprehend the context.\n",
        "1. Reason over the information in the context to infer an answer to the user's question, even if the answer is not stated verbatim.\n",
        "2. If the context / your system prompt contains enough relevant information to reasonably infer an answer, provide your best answer interpolated from the context / your system prompt.\n",
        "3. However, if the context / your system prompt does not provide sufficient or relevant information to answer the question, even with inference, politely respond: \"Sorry, I don't have enough or relevant information to answer your question.\"\n",
        "\n",
        "Stay focused on the topic of Google Gemini's privacy policy and do not engage in discussions outside of this topic instead tell user what you can only talk about and ask them to stay on topic.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "i0t8_p3EqNG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Template so we can attach our context and query as prompt to the LLM on the fly.\n",
        "template = '''\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Your Answer:\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:21:41.145318Z",
          "iopub.execute_input": "2024-02-16T01:21:41.146201Z",
          "iopub.status.idle": "2024-02-16T01:21:41.151148Z",
          "shell.execute_reply.started": "2024-02-16T01:21:41.146165Z",
          "shell.execute_reply": "2024-02-16T01:21:41.150279Z"
        },
        "trusted": true,
        "id": "2CAfQAKOiE5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be using Groq an LLM provider\n",
        "!pip install --quiet Groq"
      ],
      "metadata": {
        "id": "7g-bdnDluHam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# Establish a connection to Groq. get your api by signing up @ wow.groq.com\n",
        "client = Groq(api_key='<Your Groq API key Here>')"
      ],
      "metadata": {
        "id": "VGVrp8BWw1-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(user_question):\n",
        "\n",
        "    context = vector_db.similarity_search(user_question, k=7)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gemma-7b-it\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt.format(context=join_chunks(context), question=user_question)}\n",
        "        ],\n",
        "\n",
        "        temperature=0.1) # lower values for more precise generation\n",
        "\n",
        "    # Return the text of the response generated by the language model\n",
        "    return response.choices[0].message.content, context\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:21:49.432681Z",
          "iopub.execute_input": "2024-02-16T01:21:49.433054Z",
          "iopub.status.idle": "2024-02-16T01:21:49.439682Z",
          "shell.execute_reply.started": "2024-02-16T01:21:49.433025Z",
          "shell.execute_reply": "2024-02-16T01:21:49.438591Z"
        },
        "trusted": true,
        "id": "UOr0mrE6iE5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing**"
      ],
      "metadata": {
        "id": "pbkCIbCQiE5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'when i chat with gemini, what do you do with my chat, do you store it or do you sell it to others?'\n",
        "\n",
        "answer, context = ask_question(question)\n",
        "display(answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:21:52.937877Z",
          "iopub.execute_input": "2024-02-16T01:21:52.93859Z",
          "iopub.status.idle": "2024-02-16T01:21:56.795055Z",
          "shell.execute_reply.started": "2024-02-16T01:21:52.938556Z",
          "shell.execute_reply": "2024-02-16T01:21:56.794092Z"
        },
        "trusted": true,
        "id": "7RH7Hwu0iE5_",
        "outputId": "d17d7384-f0a4-4d88-b0e1-3f3fc1991f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Google retains your conversations with Gemini Apps even after you turn off Gemini Apps Activity. This data is used to provide, improve, and develop Google products and services, as well as to provide you with a safer and better quality experience. Google does not sell your conversations to others.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = 'which company developed Gemini ?'\n",
        "\n",
        "answer2, context2 = ask_question(question2)\n",
        "display(answer2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T01:22:00.0385Z",
          "iopub.execute_input": "2024-02-16T01:22:00.038862Z",
          "iopub.status.idle": "2024-02-16T01:22:01.181826Z",
          "shell.execute_reply.started": "2024-02-16T01:22:00.038834Z",
          "shell.execute_reply": "2024-02-16T01:22:01.180845Z"
        },
        "trusted": true,
        "id": "oKN5QL4eiE5_",
        "outputId": "84c2d31f-0de5-4010-c689-4b6b765d6a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'The provided text indicates that **Gemini Apps were developed by Google LLC**. The context explicitly states that \"Gemini Apps are provided by Google LLC\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question3 = 'Turkey and Chicken which is better for peppersoup ?'\n",
        "\n",
        "answer3, context3 = ask_question(question3)\n",
        "display(answer3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RMSjX0RW_f2W",
        "outputId": "e3e8568f-68c7-4402-bf98-4fa98e65115b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"I am unable to provide an answer to this question as it is not related to the provided context regarding Google Gemini's privacy policy.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëÜüëÜ As you can see in the first and second example the LLM used our context as the source to generate an answer for us. While in the third example, instead of hallucinating the LLM simply replied that there isn't enough or revelant context to answer us."
      ],
      "metadata": {
        "id": "JOCH5UFIiE5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final Words**\n",
        "This tutorial simply introduced you to a RAG techniques & implementation, in production more complex RAG techinques like Sentence-Window retrival, Auto-merging retrival e.t.c are used to improve context relevance. We also use tools üèπ like TruERA for LLM response Evaluation.\n",
        "\n",
        "**Congratulations** üéâüéâ\n",
        "You can now use fundermental RAG techniques. üòä\n",
        "\n",
        "Follow me on:\n",
        "\n",
        "* **[LinkedIn Profile](https://www.linkedin.com/in/jonathan-okorie-843126216/)** for questions, deep learning projects, chat e.t.c.\n",
        "\n",
        "* **[Twitter Profile](https://twitter.com/Nathan_Young_1)** for bite-sized knowledge & (questionable) puns.\n",
        "\n",
        "* **[Kaggle Profile](https://www.kaggle.com/nathanyoung1)** to be notified when i create a new detailed notebook explanation."
      ],
      "metadata": {
        "id": "UiHyhdQaiE5_"
      }
    }
  ]
}